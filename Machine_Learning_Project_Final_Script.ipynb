{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshibrom/Machine-Learning-Tweet-Classification/blob/main/Machine_Learning_Project_Final_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA64PWxUGB0A"
      },
      "outputs": [],
      "source": [
        "# Group Members: Abigail Amiscosa, Joshua Ibrom, Julian Spindola"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies as needed:\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "file_path = \"tweets.csv\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"vstepanenko/disaster-tweets\",\n",
        "  file_path,\n",
        "  # Provide any additional arguments like\n",
        "  # sql_query or pandas_kwargs. See the\n",
        "  # documenation for more information:\n",
        "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
        ")\n",
        "\n",
        "print(\"First 5 records:\", df.head())"
      ],
      "metadata": {
        "id": "uSnsZc2dGeuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training (80%) and testing (20%)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print some basic info to confirm\n",
        "print(\"Training set size:\", len(train_df))\n",
        "print(\"Testing set size:\", len(test_df))\n",
        "\n",
        "# print(\"\\nSample training data:\")\n",
        "# print(train_df.head())\n",
        "\n",
        "# print(\"\\nSample testing data:\")\n",
        "# print(test_df.head())\n"
      ],
      "metadata": {
        "id": "5RaDvTsdK-bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords (only once)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # remove URLs\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)                     # remove mentions\n",
        "    text = re.sub(r\"#\\w+\", \"\", text)                     # remove hashtags\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # remove punctuation\n",
        "    text = text.lower()                                  # lowercase\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])  # remove stopwords\n",
        "    return text.strip()\n",
        "\n",
        "# Create the new 'clean_text' column\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "nh-WsEj1LVKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Make sure your dataset is loaded\n",
        "# Example: df = train_df or the original df\n",
        "\n",
        "# Basic dataset overview\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "print(\"\\nMissing values per column:\\n\", df.isnull().sum())\n",
        "print(\"\\nClass distribution:\\n\", df['target'].value_counts())\n",
        "\n",
        "# Plot class distribution (0 = Not Disaster, 1 = Disaster)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='target', data=df)\n",
        "plt.title(\"Distribution of Disaster vs. Non-Disaster Tweets\")\n",
        "plt.xlabel(\"Target (0 = Non-Disaster, 1 = Disaster)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# Tweet length distribution\n",
        "df['text_length'] = df['text'].apply(len)\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.histplot(df['text_length'], bins=30, kde=True)\n",
        "plt.title(\"Distribution of Tweet Lengths\")\n",
        "plt.xlabel(\"Tweet Length (characters)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "# Most common words\n",
        "disaster_words = \" \".join(df[df['target'] == 1]['clean_text'])\n",
        "nondisaster_words = \" \".join(df[df['target'] == 0]['clean_text'])\n",
        "\n",
        "# Generate word clouds\n",
        "wordcloud_disaster = WordCloud(width=800, height=400, background_color='white').generate(disaster_words)\n",
        "wordcloud_non = WordCloud(width=800, height=400, background_color='white').generate(nondisaster_words)\n",
        "\n",
        "# Show them side-by-side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "axes[0].imshow(wordcloud_disaster, interpolation='bilinear')\n",
        "axes[0].set_title(\"Word Cloud ‚Äì Disaster Tweets\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(wordcloud_non, interpolation='bilinear')\n",
        "axes[1].set_title(\"Word Cloud ‚Äì Non-Disaster Tweets\")\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qpYJPvUbLx-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Data Loading and Exploration\n",
        "\n",
        "# Install dependencies if needed:\n",
        "# pip install kagglehub[pandas-datasets] torch torchvision transformers scikit-learn bert-score matplotlib\n",
        "!pip install bert-score\n",
        "\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import re\n",
        "import string\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"tweets.csv\"\n",
        "\n",
        "df = kagglehub.load_dataset(\n",
        "    KaggleDatasetAdapter.PANDAS,\n",
        "    \"vstepanenko/disaster-tweets\",\n",
        "    file_path,\n",
        ")\n",
        "\n",
        "# print(\"First 5 records:\")\n",
        "# print(df.head())\n",
        "# print(\"\\nDataset info:\")\n",
        "# print(df.info())"
      ],
      "metadata": {
        "id": "k5IYvJniNhUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Text Preprocessing\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)     # remove mentions\n",
        "    text = re.sub(r\"#\\w+\", \"\", text)     # remove hashtags\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # remove punctuation\n",
        "    text = text.lower().strip()\n",
        "    return text\n",
        "\n",
        "df[\"text\"] = df[\"text\"].astype(str).apply(clean_text)"
      ],
      "metadata": {
        "id": "G-bI0LhwQ2lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Feature Engineering & Vectorization\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"target\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# CNN BASELINE (TF-IDF)\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train.tolist()).toarray()\n",
        "X_test_tfidf = vectorizer.transform(X_test.tolist()).toarray()\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_tfidf, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test_tfidf, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32)\n",
        "\n",
        "train_dataset_cnn = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset_cnn = TensorDataset(X_test_tensor, y_test_tensor)"
      ],
      "metadata": {
        "id": "nr-9raQsQ518"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Training - CNN Baseline\n",
        "\n",
        "class CNNBaseline(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes=2):\n",
        "        super(CNNBaseline, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.fc = nn.Linear(16 * ((input_dim - 5 + 1)//2), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "cnn_model = CNNBaseline(input_dim=X_train_tfidf.shape[1])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(cnn_model.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 3\n",
        "cnn_model.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = cnn_model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"[CNN] Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Evaluate CNN\n",
        "cnn_model.eval()\n",
        "y_pred_cnn = []\n",
        "with torch.no_grad():\n",
        "    for X_batch, _ in test_loader:\n",
        "        outputs = cnn_model(X_batch)\n",
        "        y_pred_cnn.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "print(\"\\n=== CNN Evaluation ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_cnn))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_cnn))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_cnn))\n",
        "print(\"F1:\", f1_score(y_test, y_pred_cnn))\n"
      ],
      "metadata": {
        "id": "TUO6Pc4XQ7W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Training - BERT Model\n",
        "\n",
        "# Install bert-score library\n",
        "!pip install bert-score\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set checkpoint directory\n",
        "import os\n",
        "checkpoint_dir = \"/content/drive/MyDrive/Disaster_BERT_Checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Tokenizer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch, padding=True, truncation=True, max_length=64, return_tensors=\"pt\")\n",
        "\n",
        "train_encodings = tokenize(X_train.tolist())\n",
        "test_encodings = tokenize(X_test.tolist())\n",
        "\n",
        "train_dataset = TensorDataset(\n",
        "    train_encodings[\"input_ids\"],\n",
        "    train_encodings[\"attention_mask\"],\n",
        "    torch.tensor(y_train.values)\n",
        ")\n",
        "test_dataset = TensorDataset(\n",
        "    test_encodings[\"input_ids\"],\n",
        "    test_encodings[\"attention_mask\"],\n",
        "    torch.tensor(y_test.values)\n",
        ")\n",
        "\n",
        "train_loader_bert = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader_bert = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)\n",
        "optimizer = AdamW(bert_model.parameters(), lr=2e-5)\n",
        "\n",
        "# Training\n",
        "epochs = 2\n",
        "best_f1 = 0.0\n",
        "\n",
        "bert_model.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader_bert:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "        outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader_bert)\n",
        "    print(f\"[BERT] Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Evaluate after epoch\n",
        "    bert_model.eval()\n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader_bert:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "            outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
        "            preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
        "            trues.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_f1 = f1_score(trues, preds)\n",
        "    print(f\"Epoch {epoch+1} F1 Score: {epoch_f1:.4f}\")\n",
        "\n",
        "    # Save checkpoint to Drive\n",
        "    save_path = os.path.join(checkpoint_dir, f\"bert_checkpoint_epoch{epoch+1}.pt\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': bert_model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': avg_loss,\n",
        "    }, save_path)\n",
        "    print(f\"‚úÖ Saved checkpoint to Google Drive: {save_path}\")\n",
        "\n",
        "    # Save best model\n",
        "    if epoch_f1 > best_f1:\n",
        "        best_f1 = epoch_f1\n",
        "        best_path = os.path.join(checkpoint_dir, \"bert_best_model.pt\")\n",
        "        torch.save(bert_model.state_dict(), best_path)\n",
        "        print(f\"üåü New best model saved at epoch {epoch+1} (F1={best_f1:.4f})\")\n",
        "\n",
        "    bert_model.train()\n",
        "\n",
        "# Final evaluation\n",
        "bert_model.eval()\n",
        "preds, trues = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader_bert:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
        "        preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
        "        trues.extend(labels.cpu().numpy())\n",
        "\n",
        "print(\"\\n=== BERT Evaluation ===\")\n",
        "print(\"Accuracy:\", accuracy_score(trues, preds))\n",
        "print(\"Precision:\", precision_score(trues, preds))\n",
        "print(\"Recall:\", recall_score(trues, preds))\n",
        "print(\"F1:\", f1_score(trues, preds))\n",
        "\n",
        "# BERTScore (semantic similarity check)\n",
        "P, R, F1_score_val = bert_score(X_test.tolist()[:50], X_test.tolist()[:50], lang=\"en\", verbose=False)\n",
        "print(\"\\nBERTScore (self-similarity check):\", F1_score_val.mean().item())\n",
        "\n",
        "# Save final model and tokenizer to Drive\n",
        "final_dir = os.path.join(checkpoint_dir, \"bert_disaster_final\")\n",
        "bert_model.save_pretrained(final_dir)\n",
        "tokenizer.save_pretrained(final_dir)\n",
        "print(f\"‚úÖ Final fine-tuned BERT model and tokenizer saved to Google Drive at: {final_dir}\")\n"
      ],
      "metadata": {
        "id": "5HTaRqeIRApQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. BERTScore Evaluation\n",
        "\n",
        "# Extract tweets the model predicted as disaster (label = 1)\n",
        "predicted_disaster_texts = X_test[np.array(preds) == 1]\n",
        "\n",
        "# Extract tweets that are truly disaster (label = 1)\n",
        "true_disaster_texts = X_test[y_test.values == 1]\n",
        "\n",
        "sample_size = min(50, len(predicted_disaster_texts), len(true_disaster_texts))\n",
        "\n",
        "predicted_samples = predicted_disaster_texts[:sample_size].tolist()\n",
        "true_samples = true_disaster_texts[:sample_size].tolist()\n",
        "\n",
        "print(f\"\\nComputing BERTScore on {sample_size} predicted-vs-true disaster tweets...\")\n",
        "\n",
        "P, R, F1 = bert_score(predicted_samples, true_samples, lang=\"en\", verbose=True)\n",
        "\n",
        "print(\"\\n=== Semantic BERTScore Evaluation ===\")\n",
        "print(f\"Precision: {P.mean():.4f}\")\n",
        "print(f\"Recall:    {R.mean():.4f}\")\n",
        "print(f\"F1 Score:  {F1.mean():.4f}\")\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"‚Ä¢ Higher scores mean predicted disaster tweets are semantically similar to real disaster tweets.\")\n",
        "print(\"‚Ä¢ This helps evaluate whether the model is capturing disaster-related meaning, not just labels.\")\n"
      ],
      "metadata": {
        "id": "C3XEwYdRRDlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Hyperparameter Tuning & Architecture Experiments\n",
        "\n",
        "# 7.1 CNN Hyperparameter Tuning (Batch Size & LR)\n",
        "\n",
        "cnn_experiment_results = []\n",
        "\n",
        "batch_sizes = [16, 32]\n",
        "learning_rates = [0.001, 0.0005]\n",
        "\n",
        "print(\"Running CNN Hyperparameter Experiments...\\n\")\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    for lr in learning_rates:\n",
        "        print(f\"Testing CNN with batch_size={bs}, learning_rate={lr}\")\n",
        "\n",
        "        # Create DataLoaders for this experiment\n",
        "        train_loader_exp = DataLoader(train_dataset_cnn, batch_size=bs, shuffle=True)\n",
        "        test_loader_exp = DataLoader(test_dataset_cnn, batch_size=bs)\n",
        "\n",
        "        model_exp = CNNBaseline(input_dim=X_train_tfidf.shape[1]).to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer_exp = torch.optim.Adam(model_exp.parameters(), lr=lr)\n",
        "\n",
        "        # Train\n",
        "        model_exp.train()\n",
        "        for batch_x, batch_y in train_loader_exp:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            optimizer_exp.zero_grad()\n",
        "            outputs = model_exp(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer_exp.step()\n",
        "\n",
        "        # Evaluate\n",
        "        model_exp.eval()\n",
        "        preds_exp = []\n",
        "        trues_exp = []\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in test_loader_exp:\n",
        "                batch_x = batch_x.to(device)\n",
        "                outputs = model_exp(batch_x)\n",
        "                preds_exp.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "                trues_exp.extend(batch_y.numpy())\n",
        "\n",
        "        acc = accuracy_score(trues_exp, preds_exp)\n",
        "        f1 = f1_score(trues_exp, preds_exp)\n",
        "\n",
        "        cnn_experiment_results.append({\n",
        "            \"batch_size\": bs,\n",
        "            \"learning_rate\": lr,\n",
        "            \"accuracy\": acc,\n",
        "            \"f1\": f1\n",
        "        })\n",
        "\n",
        "        print(f\" -> Accuracy: {acc:.4f}, F1: {f1:.4f}\\n\")\n",
        "\n",
        "# 7.2 CNN Architecture Variation (Extra Conv Layer)\n",
        "\n",
        "print(\"\\nTesting CNN Architecture Variation: Extra Convolution Layer...\\n\")\n",
        "\n",
        "class CNNVariant(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "cnn_variant = CNNVariant(X_train_tfidf.shape[1]).to(device)\n",
        "optimizer_var = torch.optim.Adam(cnn_variant.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# train\n",
        "cnn_variant.train()\n",
        "for batch_x, batch_y in train_loader:\n",
        "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "    optimizer_var.zero_grad()\n",
        "    outputs = cnn_variant(batch_x)\n",
        "    loss = criterion(outputs, batch_y)\n",
        "    loss.backward()\n",
        "    optimizer_var.step()\n",
        "\n",
        "# evaluate\n",
        "cnn_variant.eval()\n",
        "v_preds, v_true = [], []\n",
        "with torch.no_grad():\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        batch_x = batch_x.to(device)\n",
        "        outputs = cnn_variant(batch_x)\n",
        "        v_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        v_true.extend(batch_y.numpy())\n",
        "\n",
        "variant_accuracy = accuracy_score(v_true, v_preds)\n",
        "variant_f1 = f1_score(v_true, v_preds)\n",
        "\n",
        "print(f\"Variant CNN Accuracy: {variant_accuracy:.4f}\")\n",
        "print(f\"Variant CNN F1:       {variant_f1:.4f}\")\n",
        "\n",
        "# 7.3 BERT Hyperparameter Tuning (Learning Rate)\n",
        "\n",
        "print(\"\\nRunning BERT Hyperparameter Experiments...\\n\")\n",
        "\n",
        "bert_learning_rates = [2e-5, 3e-5]\n",
        "\n",
        "bert_results = []\n",
        "\n",
        "for lr in bert_learning_rates:\n",
        "    print(f\"Testing BERT with learning_rate={lr}\")\n",
        "\n",
        "    bert_small = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",\n",
        "        num_labels=2\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer_small = torch.optim.AdamW(bert_small.parameters(), lr=lr)\n",
        "\n",
        "    # train\n",
        "    bert_small.train()\n",
        "    for batch in train_loader_bert:\n",
        "        ids, mask, labels = [x.to(device) for x in batch]\n",
        "        optimizer_small.zero_grad()\n",
        "        output = bert_small(ids, attention_mask=mask, labels=labels)\n",
        "        loss = output.loss\n",
        "        loss.backward()\n",
        "        optimizer_small.step()\n",
        "\n",
        "    # evaluate\n",
        "    bert_small.eval()\n",
        "    preds_b, trues_b = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader_bert:\n",
        "            ids, mask, labels = [x.to(device) for x in batch]\n",
        "            output = bert_small(ids, attention_mask=mask)\n",
        "            preds_b.extend(torch.argmax(output.logits, dim=1).cpu().numpy())\n",
        "            trues_b.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(trues_b, preds_b)\n",
        "    f1 = f1_score(trues_b, preds_b)\n",
        "\n",
        "    bert_results.append({\n",
        "        \"learning_rate\": lr,\n",
        "        \"accuracy\": acc,\n",
        "        \"f1\": f1\n",
        "    })\n",
        "\n",
        "    print(f\" -> Accuracy: {acc:.4f}, F1: {f1:.4f}\\n\")\n",
        "\n",
        "# 7.4 Results Summary Tables\n",
        "\n",
        "print(\"\\n===== CNN Hyperparameter Results =====\")\n",
        "cnn_results_df = pd.DataFrame(cnn_experiment_results)\n",
        "print(cnn_results_df)\n",
        "\n",
        "print(\"\\n===== CNN Architecture Variant =====\")\n",
        "print(f\"Accuracy: {variant_accuracy:.4f}, F1: {variant_f1:.4f}\")\n",
        "\n",
        "print(\"\\n===== BERT Hyperparameter Results =====\")\n",
        "bert_results_df = pd.DataFrame(bert_results)\n",
        "print(bert_results_df)\n"
      ],
      "metadata": {
        "id": "Bfnrpqp0OGuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Demo of Classifying Tweets\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# LOAD DATASET\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "file_path = \"tweets.csv\"\n",
        "\n",
        "print(\"\\nLoading dataset...\")\n",
        "df = kagglehub.load_dataset(\n",
        "    KaggleDatasetAdapter.PANDAS,\n",
        "    \"vstepanenko/disaster-tweets\",\n",
        "    file_path,\n",
        ")\n",
        "\n",
        "# Convert df.head() into list of Python dictionaries\n",
        "head_list = df.head().to_dict(orient=\"records\")\n",
        "\n",
        "# BERT PREDICTION FUNCTION\n",
        "best_model_path = \"/content/drive/MyDrive/Disaster_BERT_Checkpoints/bert_best_model.pt\"\n",
        "\n",
        "def demo_bert_best(tweet_text):\n",
        "    print(\"\\nLoading BEST model weights...\")\n",
        "\n",
        "    # 1. Build base model architecture\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",\n",
        "        num_labels=2\n",
        "    )\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # 2. Load trained state_dict\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=torch.device('cpu')))\n",
        "    model.eval()\n",
        "\n",
        "    # 3. Tokenize input\n",
        "    inputs = tokenizer(\n",
        "        tweet_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=64\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # 4. Softmax ‚Üí label + confidence\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    pred_label = torch.argmax(probs, dim=1).item()\n",
        "    confidence = probs[0][pred_label].item()\n",
        "\n",
        "    label_map = {1: \"üö® DISASTER\", 0: \"‚ùå NON-DISASTER\"}\n",
        "\n",
        "    print(f\"\\n[BEST MODEL] Tweet: {tweet_text}\")\n",
        "    print(f\"Result: {label_map[pred_label]} ({confidence:.2%})\")\n",
        "\n",
        "\n",
        "# RUN MODEL ON FIRST 5 TWEETS FROM DATASET\n",
        "\n",
        "print(\"\\n===== RUNNING MODEL ON FIRST 5 TWEETS =====\")\n",
        "for i, row in enumerate(head_list):\n",
        "    print(f\"\\n Tweet {i+1} \")\n",
        "    demo_bert_best(row[\"text\"])\n"
      ],
      "metadata": {
        "id": "Ab62BqJs-Ai8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}